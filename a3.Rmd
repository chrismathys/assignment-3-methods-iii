---
title: "Assignment 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library('pacman')
pacman::p_load(tidyverse, tidymodels, multilevelmod, rstanarm, tidybayes, broom.mixed)
```
# The report
The report for the exam, thus, consists of the answer to all the following prompts:
- Describe your machine learning pipeline. Produce a diagram of it to guide the reader (e.g. see Rybner et al 2022 Vocal markers of autism: Assessing the generalizability of ML models), and describe the different parts: data budgeting, data preprocessing, model choice and training, assessment of performance.
- Briefly justify and describe your use of simulated data, and results from the pipeline on them.
- Describe results from applying the ML pipeline to the empirical data and what can we learn from them.

Remember: plots are very very important to communicate your process and results.
```{r}
#to_do:
#   - visualise the model (draw the in logit and original scale)
#   - check whether the simulation works
#   - visualise the simulated data
#   - (*DONE) include uncertanity in your accuracy models(look slides 10)
#         - *should you add area under the ROC curve (next to accuracy) for the predictive accuracy comparison?
#   - lasso precedure to reduce the number of predictors (part III i guess)
#   - think about adding more steps to the preprocessing part
#   - visualise the priors (look slides 10)
#   - standardise cross-validation and test performance metrics and graphing (roc curve, roc_auc, accuracy, all (?))



#OPTIONAL:
#   - (optional) **just add model type (informed, sceptic) as a column so you don't have to do it every time**
#   - (optional) try to do the simulation with expand_grid() (seems very very useful)
#   - (optional) play around with the autoscale in rstanarm (sets the ratio of sd(y) to sd(x) to be constant)
#   - (optional) you could play around with the hierarchical shrinkage family ("hc")("horseshoe prior") - link: https://mc-stan.org/rstanarm/reference/priors.html

#   - 
```

# Part I - simulating the data

Use the meta-analysis reported in Parola et al (2020), create a simulated dataset with 100 matched pairs of schizophrenia and controls, each participant producing 10 repeated measures (10 trials with their speech recorded). for each of these "recordings" (data points) produce 10 acoustic measures: 6 from the meta-analysis, 4 with just random noise. Do the same for a baseline dataset including only 10 noise variables. Tip: see the slides for the code.

```{r}
simulate_data <- function(pop_effects, n = 100, n_trails = 10, individual_sd = 1, trail_sd = 0.5, error = 0.2, seed = 1){

  set.seed(seed)
    
  tibble(
    variable = map_chr(seq_along(pop_effects), ~ paste0('v_', .)), 
    population_value = pop_effects) %>% 
  mutate(id = seq(1, n, by = 1) %>% list) %>% 
    unnest(id) %>% 
  rowwise %>%
  mutate(condition = c('sz', 'hc') %>% list,
         true_effect = rnorm(1, population_value, individual_sd) / 2,
         true_effect = c(true_effect, - true_effect) %>% list,
         trail = seq(1, n_trails, by = 1) %>% list) %>% 
    unnest(c(condition, true_effect)) %>% unnest(trail) %>% 
  rowwise %>% 
  mutate(measurment = rnorm(1, true_effect, trail_sd) %>% rnorm(1, ., error),
         across(c(variable, id, condition), as_factor)) %>% 
  relocate(c(variable, population_value), .after = condition)
}
```


```{r}
m_a_values <- c(0.3, 0.4, 0.6, 0.7, 0.65, 0.45, 0.33, 0.23, 0.27, 0.31)

set.seed(1)
informed_pop_effects <- c(sample(m_a_values, 6, replace = F), rep(0, 4))


skeptic_pop_effects <- rep(0, 10)
```


```{r}
dfs_long <- map(list(informed_pop_effects, skeptic_pop_effects), simulate_data)

names(dfs_long) <- c('informed', 'skeptic')

head(dfs_long[[1]])
head(dfs_long[[2]])
```
#```{r}
#checking whether the simulation works fine

check <- map(list(informed_pop_effects, skeptic_pop_effects), ~ simulate_data(pop_effects = .x, n = 1000))



check[[1]] %>% 
  group_by(variable) %>% 
  summarise(mean = true_effect %>% mean, sd = true_effect %>% sd) %>% 
    mutate(true_mean = population_value, 
           true_sd = 1)

check[[1]] %>% 
  group_by(id) %>% 
  summarise(mean = measurment %>% mean, sd = measurment %>% sd) %>% 
    mutate(true_mean = true_effect)
#```


```{r}
#visualising the simulated data


map(dfs_long,
    ~ .x %>% 
      ggplot(aes(x = measurment, fill = condition)) +
        geom_density()+
        facet_wrap(vars(variable))
)
```

```{r}
dfs_wide <- map(dfs_long,
  ~ .x %>% 
      pivot_wider(id_cols = c(id, trail, condition),
                  names_from = variable,
                  values_from = measurment)
)
head(dfs_wide[[1]])
head(dfs_wide[[2]])
```
# Part II - machine learning pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).


1) (done) budget

2) (done*) pre-process(center, scale, etc.)
      - you can probably add some more steps
      
3) (done*) fit the model
      - you probably should visualise the predictive prior distributions (look up the lecture)
      - do convergance checks
      
4) access performance
      1. looic
      2. folds
      3. tests
      4. graphs comparing informed and sceptic
      
5) conclusions: is performance as expected, is feature importance as expected
  - check out the lecture for what he means here


## Budgeting the data:
```{r}
splits <- map(dfs_wide, ~ .x %>% initial_split(prop = 4/5))
  
dfs_training <- map(splits, ~ .x %>% training)
dfs_testing <- map(splits, ~ .x %>% testing)

rm(splits)
```
```{r}
prior_b <- normal(location = c(rep(0, 10)), scale = c(rep(0.3, 10)))
prior_intercept <- normal(0, 1)


prior_model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             prior_PD = T,
             cores = 3)
  
  
model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)
#priors!!!
```

## Preprocessing the data
```{r}
recipes <- map(dfs_training, 
               ~ recipe(condition ~ 1 + ., data = .x) %>%
                    update_role(id, trail, new_role = 'id') %>%
                    step_normalize(all_numeric())
)

wflows <- map(recipes,
              ~  workflow() %>% 
                  add_model(model) %>%  
                  add_recipe(.x)
)

prior_wflows <- map(recipes, 
                       ~ workflow() %>%
                          add_model(prior_model) %>%
                          add_recipe(.x))


 
```

## Fitting (training) the models
```{r}

prior_models <- list(prior_wflows[[1]] %>% fit(dfs_training[[1]]),
                     prior_wflows[[2]] %>% fit(dfs_training[[2]])
                     ) %>% map(extract_fit_engine)


fitted <- list(wflows[[1]] %>% fit(dfs_training[[1]]),
               wflows[[2]] %>% fit(dfs_training[[2]])
               )
names(fitted) <- c('Informed', 'Sceptic')

fitted_models <- fitted %>% map(extract_fit_engine)

rm(prior_wflows)
```



```{r}
pp_update_plot <- function(prior_model, posterior_model){
  df_draws <- 
    bind_rows(
      bind_rows(
        prior_model %>% gather_draws(`(Intercept)`),
        prior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'prior'),
      
      bind_rows(
        posterior_model %>% gather_draws(`(Intercept)`),
        posterior_model %>% gather_draws(`v_.*`, regex = T)
        ) %>% 
        mutate(type = 'posterior')
      )
  
  df_draws <- df_draws %>% 
    group_by(.variable) %>% 
    mutate(upp_lim = if_else((max(.value) + min(.value)) > 0, max(.value), - min(.value)),
           low_lim = - upp_lim) %>% 
    ungroup
  
  
  
  df_draws %>%  
    ggplot(aes(x = .value, fill = type)) +
      geom_density(alpha = 0.8) +
      labs(fill = element_blank()) +
      xlim(df_draws$low_lim[[1]], df_draws$upp_lim[[1]]) +
      facet_grid(vars(df_draws$.variable)) +
      theme_minimal() +
      theme(axis.ticks.y = element_blank(), 
            axis.text.y = element_blank())
}
```

```{r fig.width=10}
pp_update_plot(prior_models[[1]], fitted_models[[1]])+
  ggtitle('Informed')
```


```{r fig.width=10}
pp_update_plot(prior_models[[2]], fitted_models[[2]])+
  ggtitle('Sceptic')

```

































## Accessing model performance

### LOOIC #not sure if that's necessary

### Cross-validation
```{r}
folds <- map(dfs_training, ~ vfold_cv(.x, v = 8))



cv_data <- map2(wflows, folds, ~ fit_resamples(.x, .y))

cv_results <- map(cv_data, ~ collect_metrics(.x) %>% 
                    mutate(model = names(.x) %>% list,
                           upper = mean + std_err,
                           lower = mean - std_err))
```


```{r}
bind_rows(
    cv_results[[1]] %>% mutate(model = 'informed'),
    cv_results[[2]] %>% mutate(model = 'sceptic')
  ) %>% 
  ggplot(aes(x = mean, y = model, xmax = upper, xmin = lower, colour = model)) +
    geom_pointrange()+
    facet_wrap(vars(.metric)) +
    geom_vline(xintercept = 0.5, colour = 'darkred', linetype = 'dashed', alpha = 0.7) +
    theme_minimal()
```


### Test data
```{r}
test_preds <- map2(fitted, dfs_training, ~ augment(.x, .y))


map2(test_preds, names(test_preds),
     ~ .x %>% 
          roc_curve(truth = condition, .pred_sz) %>% 
            autoplot + 
            ggtitle(.y)
)

```
## Conclusions (is performance and feature importance as expected)

```{r}
#without uncertanity

test_results <- map2_df(test_preds, names(test_preds),
             ~ bind_rows(
                  .x %>% roc_auc(truth = condition, .pred_sz),
                  .x %>% accuracy(truth = condition, .pred_class)
             ) %>% 
               mutate(Model = .y)
)


test_results %>% 
  ggplot(aes(x = Model, y = .estimate, fill = Model)) +
    geom_bar(stat = 'identity') +
    facet_wrap(vars(.metric)) +
    labs(y = NULL) +
    theme_minimal()

```



```{r}
#with the uncertanity 

test_results <- tibble(draw = NULL,
                       accuracy = NULL,
                       model = NULL)


for (i in seq_along(fitted_models)){
  
  m <- fitted_models[[i]]
  name <- names(fitted_models)[[i]]
  
  draws_matrix <- posterior_epred(m)
  
  preds <- map(
    .x = draws_matrix %>% split(row(draws_matrix)), 
    .f = ~ if_else(.x > 0.5, 'hc', 'sz') %>% as_factor %>% relevel('sz')
    )
  
       
  accuracies <- map(preds,
                    ~ accuracy(dfs_training[[1]], truth = condition, estimate = .x) %>% 
                      pull(.estimate)
                    )
  

  d <- tibble(draws = seq(length(preds)),
              accuracy = accuracies, 
              model = name) %>% 
    unnest(accuracy)
    
  test_results <- bind_rows(
    test_results,
    d
  )
}
rm(i, m, name, accuracies, d)



test_results_summary <- test_results %>% 
  group_by(model) %>% 
  summarise(mean = mean(accuracy), se = sd(accuracy),
            #because we're dealing the the estimates of the population parameters, the sd already is the standard error (or at least so my limited understanding goes)
            lower = mean - 1.96*se, 
            upper = mean + 1.96*se)



#try to do this for the cross_validation as well
test_results %>%
    ggplot(aes(x = model, y = accuracy, colour = model)) +
      geom_point(alpha = 0.7) +
      geom_hline(aes(yintercept = 0.5), color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      theme_minimal()
   
test_results_summary %>%
    ggplot(aes(x = model, y = mean, ymin = lower, ymax = upper, colour = model)) +
      geom_pointrange() +
      geom_hline(aes(yintercept = 0.5), color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      labs(y = 'Accuracy') +
      theme_minimal()

```





























```{r}
matrix <- posterior_epred(fitted_models[[1]])

tibble <- posterior_epred(fitted_models[[1]]) %>% 
  as_tibble


# do it just for the informed one:
results <- tibble(seq(1, 4000, by = 1)
  
for (i in seq(1, 4000, by = 1)){
  pred_class <- posterior_epred(fitted_model[[1]])[i, ] %>% if_else(.x > 0.5, 'sz', 'hc')
  
  accuracy <- accuracy(truth_training$condition, estimate = pred_condition)
  }
  





map(if_else(.x > 0.5, 'sz', 'hc')
  accuracy(truth = dfs_training$conditon, estimate = pred_condition)
)
  
#add different fill for test and cross-validation  
  
 results %>% 
   group_by(draw) %>% 
      ggplot(aes(x = model, y = accuracy, fill = method, yintercept = 0.5)) +
        geom_point(alpha = 0.7) +
        geom_hline(color = 'darkred') +
        theme_minimal()
   
)
```

# Part III

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed. Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).


