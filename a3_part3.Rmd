---
title: "a3_part3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library('pacman')
pacman::p_load(tidyverse, tidymodels, multilevelmod, rstanarm, tidybayes, broom.mixed)
```
# Part III

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed. Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).


1. Describe and plot the data
    
      - NEW: remember about the feedback he gave for assignment ii

2. Repeat of part 2 (the same pipeline, different data)
      
        1) budget
              - **NEW: balance the train and test on demographic data (strata argument)**
        2) pre-process(center, scale, etc.)
              - **NEW: feature selection** (as separate point, look below)
        3) fit the model
              
        4) access performance
              - cross-validation
              - test set
              - graphs
              
        5) conclusions: 
          5.1) performance
          5.2) feature importance


3. Feature selection
    1. Demographic data:
        - Delete it
              OR
        - (optional) make sort of a null model using it and then compare it to the acustic features one
    2. Acustic features
        - Just removing less important features
            - lasso 
            - removing highly correlated ones
          
        - Reducing the dimentionality (PCA or SVD(?))
        
        
        compare lasso, vs corralated vs. pca
        
```{r}
rm(list = setdiff(ls(), lsf.str())) 
# removing all objects except functions from the global environment
```
      
```{r}
data_raw <- read_csv('real_data.csv')

glimpse(data_raw)
```

```{r}
data <- data_raw %>%
  rename_with(.cols = everything(), str_to_lower) %>% 
  rename(id = patid,
         condition = diagnosis) %>% 
  mutate(across(where(is.character), str_to_lower),
         across(1:7, as_factor),
         condition = if_else(condition != 'ct', 'sz', 'hc') %>% as_factor %>% relevel('sz')) %>% 
  select(-newid)

data$language %>% summary
data$corpus %>% summary

data <- data %>% 
  select(-language)


head(data)
```
## Describing the data
### Condition
```{r}
data %>% 
  count(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```

### Gender
```{r}
data %>% 
  count(gender) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))

# pct should have grouped n in the denominator
data %>% 
  count(gender, condition) %>% 
  group_by(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```
```{r}
data %>% 
  count(corpus) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))


data %>% 
  count(condition, corpus) %>% 
  group_by(condition) %>% 
  mutate(pct = n / sum(n), pct = pct %>% round(2))
```

## Modeling the data
### Budgeting
```{r}
data_background <- data %>% select(1:5)
data <- data %>% select(-c(gender, corpus))

split <- initial_split(data, prop = 4/5)

data_training <- training(split)
data_testing <- testing(split)

rm(split)
```
## Preprocessing the data
```{r}
recipes <- list()

recipes[[1]] <- recipe(condition ~ 1 + ., data = data_training) %>%
                    update_role(id, trial, new_role = 'id') %>%
                    step_normalize(all_numeric())


recipes[[2]] <- recipes[[1]] %>% step_corr(all_predictors())
recipes[[3]] <- recipes[[1]] %>% step_pca(all_predictors())
recipes[[4]] <- recipes[[1]] %>% step_umap

names(recipes) <- c('lasso', 'corr', 'pca')


# Right now you need to do this only with corr and pca 
recipes <- recipes[2:3]
#remove this later
```
### Creating the models
```{r}
prior_b <- normal(location = 0, scale = 0.3)
prior_intercept <- normal(0, 1)


model_prior <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             prior_PD = T,
             cores = 3)
  
  
model <- logistic_reg() %>% 
  set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)

model_lasso <- logistic_reg(penalty = 0.01, mixture = 1) %>% 
    set_engine('stan',
             prior = prior_b,
             prior_intercept = prior_intercept,
             cores = 3)

```
### Workflows
```{r}
wflows <- map(recipes,
               ~ workflow() %>% 
                  add_model(model) %>% 
                  add_recipe(.x))
#un # this after you decide what to do about the lasso regression

#wflows[[1]] <- workflow() %>% 
 #   add_model(model_lasso) %>% 
  #  add_recipe(recipes[[1]])
```
### Fitting the models
```{r}
set.seed(1)
fitted <- map(wflows,
              ~ .x %>% fit(data_training))

fitted_models <- map(fitted, extract_fit_engine)

set.seed(1)
prior_fitted <- map(recipes,
  ~ workflow() %>%
     add_model(model_prior) %>%
     add_recipe(.x) %>% 
     fit(data_training) %>% 
     extract_fit_engine()
)

```
### Convergance checks
```{r}

convergance_plots <- map2(
  fitted_models, 
  names(fitted_models), 
  function(.x, .y){
    list(
      plot(.x, 'trace', pars = '(Intercept)'),
      #think about which estimates to include and add this here
      plot(.x, 'neff'),
      plot(.x, 'rhat')
      ) %>%
    map(function(.x){.x + ggtitle(.y)})
  }
)

convergance_plots %>% print

rm(convergance_plots)

```

```{r}

variables_corr <- get_variables(fitted_models[[1]]) %>%
    str_subset('.*__', negate = T) %>%
      #removing all the 'technical' variables (e.g. 'treedepth__', 'stepsize__')
    str_subset('(Intercept)', negate = T) 

variables_corr <- c(
  '(Intercept)',
  variables_corr %>% str_subset('mcep.*') %>% sample(3, replace = F),
  variables_corr %>% str_subset('hmpdm.*') %>% sample(3, replace = F),
  variables_corr %>% str_subset(.,'mcep.*|hmpd.*', negate = T) %>% sample(3, replace = F)
)




variables_pca <- get_variables(fitted_models[[2]]) %>% str_subset('.*__', negate = T)
```


```{r}
pp_update_plot <- function(prior_model, posterior_model, variables){
  
  df_draws <-  bind_rows(
    prior_model %>% gather_draws(!!!syms(variables))%>% 
        mutate(type = 'prior'),
      
      posterior_model %>% gather_draws(!!!syms(variables))%>% 
        mutate(type = 'posterior')
      )
  
  df_draws <- df_draws %>% 
    group_by(.variable) %>% 
    mutate(upp_lim = if_else((max(.value) + min(.value)) > 0, max(.value), - min(.value)),
           low_lim = - upp_lim) %>% 
    ungroup
  
  
  
  df_draws %>%  
    ggplot(aes(x = .value, fill = type)) +
      geom_density(alpha = 0.7) +
      labs(fill = element_blank()) +
      xlim(df_draws$low_lim[[1]], df_draws$upp_lim[[1]]) +
      facet_grid(vars(df_draws$.variable)) +
      theme_minimal() +
      theme(axis.ticks.y = element_blank(), 
            axis.text.y = element_blank())
}
```
```{r fig.height=8}
pp_update_plot(prior_fitted[[1]], fitted_models[[1]], variables_corr)

pp_update_plot(prior_fitted[[2]], fitted_models[[2]], variables_pca)

# they look quite weird, check whether the values of the priors and posteriors much the priors you set and the models estimates (or check by running brms) :c
```


```{r}
test_preds <- map(fitted, ~ augment(.x, data_training))

map2(test_preds, names(test_preds),
     ~ .x %>% 
          roc_curve(truth = condition, .pred_sz) %>% 
            autoplot + 
            ggtitle(.y)
)


#why this one and part 2 calculate the probability for different classes??? part 2 (for sz and this one for hc)
```
```{r}
#with the uncertanity 

test_results <- tibble(draw = NULL,
                       f1 = NULL,
                       model = NULL)


for (i in seq_along(fitted_models)){
  
  m <- fitted_models[[i]]
  name <- names(fitted_models)[[i]]
  
  draws_matrix <- posterior_epred(m)
  
  roc_aucs <- map_dbl(
    draws_matrix %>% split(row(draws_matrix)),
    ~ roc_auc_vec(truth = data_training$condition, estimate = .x)
    )
  roc_aucs <- tibble(
    value = roc_aucs,
    metric = 'roc_auc',
    draw = seq_along(nrow)
    )
  
  
  preds_class <- map(
    draws_matrix %>% split(row(draws_matrix)), 
    ~ if_else(.x < 0.5, 'sz', 'hc') %>% as_factor %>% relevel('sz')
    )
  
       
  fs <- map_dbl(
    preds_class,
    ~ f_meas_vec(truth = data_training$condition, estimate = .x)
    )
  fs <- tibble(
    value = fs,
    metric = 'f1',
    draw = seq_along(nrow)
    )

  
  test_results <- bind_rows(
    test_results,
    bind_rows(fs, roc_aucs) %>% mutate(model = name)
  )
}
rm(i, m, name, draws_matrix, roc_aucs, preds_class, fs)


test_results <- test_results %>%
  mutate(value = if_else(metric == 'roc_auc', 1 - value, value))

test_results_summary <- test_results %>% 
  group_by(model, metric) %>% 
  summarise(mean = mean(value), std_err = sd(value),
            #because we're dealing the the estimates of the population parameters, the sd already is the standard error (or at least so my limited understanding goes)
            lower = mean - 1.96*std_err, 
            upper = mean + 1.96*std_err)

test_results %>%
    ggplot(aes(x = model, y = value, colour = model)) +
      geom_point(alpha = 0.7) +
      geom_hline(yintercept = 0.5, color = 'darkred', linetype = 'dashed', alpha = 0.7) +
      theme_minimal() +
      facet_wrap(vars(metric))
#the 'hc' and 'sz' seem to be flipped around
```
```{r}

```

